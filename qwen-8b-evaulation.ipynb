{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install sentencepiece accelerate bert_score rouge_score evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T05:37:37.633132Z","iopub.execute_input":"2025-05-12T05:37:37.633384Z","iopub.status.idle":"2025-05-12T05:38:49.209760Z","shell.execute_reply.started":"2025-05-12T05:37:37.633362Z","shell.execute_reply":"2025-05-12T05:38:49.209046Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\nCollecting bert_score\n  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\nCollecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nCollecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.16)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate) (2024.2.0)\nDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=104edfdb4a1cf38a6ff2900ed69c331451a3613887b996d2f491e93c217bf8e0\n  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\nSuccessfully built rouge_score\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, rouge_score, evaluate, bert_score\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bert_score-0.3.13 evaluate-0.4.3 fsspec-2024.12.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Sample input\ninput_text = \"Summarize the following bill: California Assembly Bill 5 (AB 5) affects how workers are classified as employees or independent contractors. It updates the criteria to determine employment status and expands the responsibilities of employers. The bill requires employers to prove that workers are independent contractors and establishes a presumption that workers are employees unless employers can demonstrate otherwise. AB 5 impacts a wide range of industries, including tech, gig economy, and freelance sectors. The bill aims to protect workers' rights and ensure they receive benefits and protections afforded to employees. However, it has also faced criticism for potentially limiting business flexibility and hindering the growth of the gig econom\"\n\n# Tokenize input\ninputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n\n# Generate response (fix: disable use_cache)\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=200,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n        use_cache=False  \n    )\n\n# Decode and print output\nresponse = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Model response:\", response)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T01:53:22.505781Z","iopub.execute_input":"2025-05-12T01:53:22.506085Z","iopub.status.idle":"2025-05-12T01:53:44.819315Z","shell.execute_reply.started":"2025-05-12T01:53:22.506062Z","shell.execute_reply":"2025-05-12T01:53:44.818651Z"}},"outputs":[{"name":"stdout","text":"Model response: Summarize the following bill: California Assembly Bill 5 (AB 5) affects how workers are classified as employees or independent contractors. It updates the criteria to determine employment status and expands the responsibilities of employers. The bill requires employers to prove that workers are independent contractors and establishes a presumption that workers are employees unless employers can demonstrate otherwise. AB 5 impacts a wide range of industries, including tech, gig economy, and freelance sectors. The bill aims to protect workers' rights and ensure they receive benefits and protections afforded to employees. However, it has also faced criticism for potentially limiting business flexibility and hindering the growth of the gig economy.\n\n### Answer: California Assembly Bill 5 (AB 5) updates and expands criteria for classifying workers as employees or independent contractors, shifting the burden onto employers to prove independent contractor status. It affects various industries, including tech and gig economy sectors, and aims to protect workers' rights to benefits and protections.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset\nimport evaluate\nimport torch\nfrom peft import PeftModel\nimport time\nimport gc\nimport os\n\n# Hugging Face token\ntoken = \"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\"\n\n# Model IDs\nbase_model_id = \"Qwen/Qwen3-0.6B\"\nadapter_id = \"NLUandHelwan/qwen3-0.6b-california-bill-lora\"\n\n# Check CUDA availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif device.type == \"cuda\":\n    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nelse:\n    print(\"CUDA not available. Using CPU.\")\n\n# Load tokenizer\nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_id, token=token, use_fast=True)\n\n# Load base model in half-precision for speed\nprint(\"Loading base model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    base_model_id,\n    torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32,\n    trust_remote_code=True,\n    token=token\n).to(device)\n\n# Load LoRA adapter\nprint(\"Applying LoRA adapter...\")\nmodel = PeftModel.from_pretrained(base_model, adapter_id, token=token).to(device)\n\n# Set to evaluation mode for inference\nmodel.eval()\nprint(f\"Model loaded on: {next(model.parameters()).device}\")\n\n# Load 100 examples from the dataset\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"billsum\", split=\"test[:50]\")\nprint(f\"Loaded {len(dataset)} examples\")\n\n# Prompt template focused on summary only\nprompt_template = \"\"\"Below is a legal document that needs to be summarized. Create a concise summary that captures the key points and implications of the document.\n\n### Legal Document:\n{}\n\n### Summary:\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T05:38:49.211310Z","iopub.execute_input":"2025-05-12T05:38:49.211525Z","iopub.status.idle":"2025-05-12T05:39:37.643093Z","shell.execute_reply.started":"2025-05-12T05:38:49.211509Z","shell.execute_reply":"2025-05-12T05:39:37.642490Z"}},"outputs":[{"name":"stderr","text":"2025-05-12 05:39:02.330519: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747028342.534084      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747028342.590189      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using GPU: Tesla T4\nTotal GPU memory: 15.83 GB\nLoading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/9.68k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e81d87508c04309b4fafe2b20afb529"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cf0d0dc23fa47cd8ae99265809894d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55156a78044b4e7b8e7e2cc9c12861ee"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8e03e0135c04f36a7802b1ec9d61b48"}},"metadata":{}},{"name":"stdout","text":"Loading base model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca4503f56534589a66b623afeb4da62"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"294f32d81de84e96af28b0b90a11cf00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43440c57128448eca3bc64dfc4507e3c"}},"metadata":{}},{"name":"stdout","text":"Applying LoRA adapter...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/789 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fde7595f1126412da4424df7baf7e2ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/323M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3658a1e584b9467db8f1e137da10532e"}},"metadata":{}},{"name":"stdout","text":"Model loaded on: cuda:0\nLoading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfa96682d3dc40f8b046035f281a88ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/91.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"103cd7e1d2fe4fc897f69c6c34929e97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/15.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"264e7bae141d4e55ac4aee706cd41d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ca_test-00000-of-00001.parquet:   0%|          | 0.00/6.12M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7a69b8d18164a9ba56b0726246e0002"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/18949 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4c2d02470b4c55b3b52bab86d1c5d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3269 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72d52ded6f804dd79310e7d6f9f49541"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating ca_test split:   0%|          | 0/1237 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f27c125e8ed64fb3bf3ac8e3d887df02"}},"metadata":{}},{"name":"stdout","text":"Loaded 50 examples\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"def extract_summary(text):\n    if \"### Summary:\" in text:\n        return text.split(\"### Summary:\")[1].strip()\n    elif \"###\" in text:\n        parts = text.split(\"###\")\n        if len(parts) > 2:\n            return parts[2].strip()\n    elif prompt_template.split(\"\\n\\n\")[-1] in text:\n        return text.split(prompt_template.split(\"\\n\\n\")[-1])[1].strip()\n    return text\n\n# Optimized generation function\ndef generate_summaries(dataset, batch_size=10):\n    all_summaries = []\n    all_times = []\n    total_tokens = 0\n    \n    # Optimize for inference\n    with torch.cuda.amp.autocast(enabled=True), torch.no_grad():\n        for i in range(0, len(dataset), batch_size):\n            batch_start = time.time()\n            print(f\"Processing batch {i//batch_size + 1}/{(len(dataset) + batch_size - 1)//batch_size}\")\n            batch = dataset[i:i+batch_size]\n            \n            prompts = [prompt_template.format(doc) for doc in batch[\"text\"]]\n            \n            # Pre-compile for better performance\n            inputs = tokenizer(\n                prompts, \n                return_tensors=\"pt\", \n                padding=True, \n                truncation=True, \n                max_length=8192\n            ).to(model.device)\n            \n            # Use more efficient generation settings\n            start_time = time.time()\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=512,\n                do_sample=False,  # Greedy decoding \n                use_cache=True,  \n                num_beams=1,\n                pad_token_id=tokenizer.eos_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n            )\n            end_time = time.time()\n            \n            # Track generation stats\n            batch_time = end_time - start_time\n            batch_input_tokens = inputs.input_ids.numel()\n            batch_output_tokens = outputs.numel() - batch_input_tokens\n            total_tokens += batch_output_tokens\n            all_times.append(batch_time)\n            tokens_per_second = batch_output_tokens / batch_time\n            print(f\"Batch generated {batch_output_tokens} tokens in {batch_time:.2f}s ({tokens_per_second:.2f} tokens/sec)\")\n            \n            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n            summaries = [extract_summary(text) for text in decoded]\n            all_summaries.extend(summaries)\n            \n            # Explicitly free memory\n            del inputs, outputs, decoded\n            torch.cuda.empty_cache()\n            gc.collect()\n            \n            batch_end = time.time()\n            print(f\"Total batch processing time: {batch_end - batch_start:.2f}s\")\n            \n    # Calculate performance metrics\n    avg_time = sum(all_times) / len(all_times)\n    avg_tokens_per_second = total_tokens / sum(all_times)\n    print(f\"\\nPerformance Summary:\")\n    print(f\"Total generation time: {sum(all_times):.2f}s\")\n    print(f\"Average batch time: {avg_time:.2f}s\")\n    print(f\"Average throughput: {avg_tokens_per_second:.2f} tokens/sec\")\n    \n    return all_summaries\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Starting generation...\")\nprint(\"=\"*50)\n\n# Time the entire process\nstart_time = time.time()\ngenerated_summaries = generate_summaries(dataset)\ntotal_time = time.time() - start_time\nprint(f\"\\nTotal execution time: {total_time:.2f}s\")\n\n# Save raw generations and reference summaries\nprint(\"Saving results...\")\nwith open(\"generated_summaries.txt\", \"w\", encoding=\"utf-8\") as f:\n    for summary in generated_summaries:\n        f.write(f\"{summary}\\n\\n---\\n\\n\")\n\nwith open(\"reference_summaries.txt\", \"w\", encoding=\"utf-8\") as f:\n    for summary in dataset[\"summary\"]:\n        f.write(f\"{summary}\\n\\n---\\n\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T05:39:37.643830Z","iopub.execute_input":"2025-05-12T05:39:37.644518Z","iopub.status.idle":"2025-05-12T05:49:25.496290Z","shell.execute_reply.started":"2025-05-12T05:39:37.644498Z","shell.execute_reply":"2025-05-12T05:49:25.495715Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nStarting generation...\n==================================================\nProcessing batch 1/5\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3720838413.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=True), torch.no_grad():\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Batch generated 5120 tokens in 127.71s (40.09 tokens/sec)\nTotal batch processing time: 128.32s\nProcessing batch 2/5\nBatch generated 5120 tokens in 111.52s (45.91 tokens/sec)\nTotal batch processing time: 112.03s\nProcessing batch 3/5\nBatch generated 5120 tokens in 86.64s (59.10 tokens/sec)\nTotal batch processing time: 87.10s\nProcessing batch 4/5\nBatch generated 5120 tokens in 109.25s (46.87 tokens/sec)\nTotal batch processing time: 109.72s\nProcessing batch 5/5\nBatch generated 5120 tokens in 150.16s (34.10 tokens/sec)\nTotal batch processing time: 150.66s\n\nPerformance Summary:\nTotal generation time: 585.29s\nAverage batch time: 117.06s\nAverage throughput: 43.74 tokens/sec\n\nTotal execution time: 587.84s\nSaving results...\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Evaluate using ROUGE\nprint(\"Evaluating summaries...\")\nrouge = evaluate.load(\"rouge\")\nresults = rouge.compute(\n    predictions=generated_summaries,\n    references=dataset[\"summary\"],\n    use_aggregator=True\n)\n\nprint(\"\\nROUGE Evaluation Results:\")\nfor metric, score in results.items():\n    print(f\"{metric}: {score:.4f}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T05:52:17.578626Z","iopub.execute_input":"2025-05-12T05:52:17.578948Z","iopub.status.idle":"2025-05-12T05:52:19.970384Z","shell.execute_reply.started":"2025-05-12T05:52:17.578914Z","shell.execute_reply":"2025-05-12T05:52:19.969797Z"}},"outputs":[{"name":"stdout","text":"Evaluating summaries...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f84bd119f14010892464213483d484"}},"metadata":{}},{"name":"stdout","text":"\nROUGE Evaluation Results:\nrouge1: 0.2842\nrouge2: 0.1252\nrougeL: 0.1929\nrougeLsum: 0.2418\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# BERT Score evaluation for semantic similarity\nprint(\"\\nCalculating BERTScore...\")\nbertscore = evaluate.load(\"bertscore\")\nbert_results = bertscore.compute(\n    predictions=generated_summaries,\n    references=dataset[\"summary\"], \n    lang=\"en\",\n    model_type=\"distilbert-base-uncased\"\n)\n\nprint(\"\\nBERTScore Results:\")\nprint(f\"Precision: {sum(bert_results['precision'])/len(bert_results['precision']):.4f}\")\nprint(f\"Recall: {sum(bert_results['recall'])/len(bert_results['recall']):.4f}\")\nprint(f\"F1: {sum(bert_results['f1'])/len(bert_results['f1']):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T05:52:20.528475Z","iopub.execute_input":"2025-05-12T05:52:20.528721Z","iopub.status.idle":"2025-05-12T05:52:28.085542Z","shell.execute_reply.started":"2025-05-12T05:52:20.528701Z","shell.execute_reply":"2025-05-12T05:52:28.084736Z"}},"outputs":[{"name":"stdout","text":"\nCalculating BERTScore...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4519e338d1df47929caf29312a55cdc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4356f92e978642f5a8317d4803e50f82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b39e38c85bad423694cac56787079a46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4ea324fbe2446a28cd6edef0f1c02ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fca8ec394df43718a698d5acc8c8bce"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d46e1f16000a4e0c96847b84ae090efb"}},"metadata":{}},{"name":"stdout","text":"\nBERTScore Results:\nPrecision: 0.7357\nRecall: 0.7818\nF1: 0.7561\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}